# Contenedores para DropTools (Scraper + DB + GUI)
# (version ya no es necesario en Docker Compose v2+)
services:
  # 1. Base de Datos PostgreSQL con pgvector
  db:
    image: pgvector/pgvector:pg17
    container_name: droptools_db
    restart: always
    env_file:
      - .env.docker
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./docs/droptools_db.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5433:5432"
    networks:
      - droptools_net

  # 2. Interfaz Gráfica para DB (pgAdmin 4)
  pgadmin:
    image: dpage/pgadmin4
    container_name: droptools_pgadmin
    restart: always
    env_file:
      - .env.docker
    ports:
      - "5050:80"
    depends_on:
      - db
    networks:
      - droptools_net

  # 3. Scraper (Python + Selenium)
  scraper:
    build:
      context: .
      target: selenium
    container_name: droptools_scraper
    # Comparte variables del .env con Python
    env_file:
      - .env.docker
    volumes:
      - ./raw_data:/app/raw_data # Persistir JSONs y fotos descargadas
      - ./backend:/app/backend # Código fuente actualizado
      - ./logs:/app/logs
    # Límites de recursos AUMENTADOS
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    shm_size: '1g' # Aumentado de 512m a 1 GB (Chrome necesita más)
    # No inicia solo, tú lo lanzas cuando quieras scrapear
    profiles: [ "workers" ]
    networks:
      - droptools_net
    # Comando por defecto (opcional, o se corre manual)
    command: python backend/manage.py scraper
    restart: on-failure:3 # Auto-reinicio en caso de error
    # depends_on:
    #  - db

  loader:
    build:
      context: .
      target: selenium
    container_name: droptools_loader
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./raw_data:/app/raw_data
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 500M
    networks:
      - droptools_net
    command: python backend/manage.py loader
    profiles: [ "workers" ]
    restart: on-failure:3 # Auto-reinicio en caso de error
    # depends_on:
    #   - db

  vectorizer:
    build:
      context: .
      target: vectorizer
    container_name: droptools_vectorizer
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./cache_huggingface:/app/cache_huggingface
      - ./logs:/app/logs
    # ⚡ Habilitar acceso a la GPU NVIDIA
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - droptools_net
    command: python backend/manage.py vectorizer
    profiles: [ "workers" ]
    restart: on-failure:3 # Auto-reinicio en caso de error
    # depends_on:
    #   - db

  clusterizer:
    build:
      context: .
      target: selenium # Reusamos la imagen que ya tiene python y librerias
    container_name: droptools_clusterizer
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    networks:
      - droptools_net
    command: python backend/manage.py clusterizer
    profiles: [ "workers" ]
    restart: no
    # depends_on:
    #   - db

  classifier:
    build:
      context: .
      target: selenium
    container_name: droptools_classifier
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.2G
    networks:
      - droptools_net
    command: python backend/manage.py classifier
    profiles: [ "workers" ]
    restart: no
    # depends_on:
    #   - db

  classifier_2:
    build:
      context: .
      target: selenium
    container_name: droptools_classifier_2
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1.2G
    networks:
      - droptools_net
    command: python backend/manage.py classifier
    profiles: [ "workers" ]
    restart: no
    # depends_on:
    #   - db

  market_trender:
    build:
      context: .
      target: selenium
    container_name: droptools_market_trender
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    networks:
      - droptools_net
    command: python backend/manage.py market_trender
    profiles: [ "workers" ]
    restart: no
    # depends_on:
    #   - db

  meta_scholar:
    build:
      context: .
      target: selenium
    container_name: droptools_meta_scholar
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
      - ./cache_huggingface:/app/cache_huggingface
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    networks:
      - droptools_net
    command: python backend/manage.py meta_scholar
    profiles: [ "workers" ]
    restart: "no"
    depends_on:
      #   - db
      - backend

  shopify_auditor:
    build:
      context: .
      target: selenium
    container_name: droptools_shopify_auditor
    env_file:
      - .env.docker
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
      - ./raw_data:/app/raw_data
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    networks:
      - droptools_net
    command: python backend/manage.py shopify_auditor
    profiles: [ "workers" ]
    restart: no
    # depends_on:
    #   - db

    # 5. Backend API (Django REST Framework)
  backend:
    build:
      context: .
      target: selenium
    container_name: droptools_backend
    restart: always
    env_file:
      - .env.docker
    environment:
      # Modo: development (con override) o production. Si no usas override, pon aquí DROPTOOLS_ENV=production
      - DROPTOOLS_ENV=development
      # Hot-reload y desarrollo
      - PYTHONUNBUFFERED=1
      - PYTHONIOENCODING=utf-8
      - PYTHONUTF8=1
      - DJANGO_SETTINGS_MODULE=droptools_backend.settings
      - PYTHONPATH=/app/backend
      # Celery configuration
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      # Evitar caché de Python para desarrollo
      - PYTHONDONTWRITEBYTECODE=1
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
      - ./cache_huggingface:/app/cache_huggingface
      - /var/run/docker.sock:/var/run/docker.sock # Control Docker from within container
      # Excluir __pycache__ para evitar problemas de sincronización
      - /app/backend/**/__pycache__
    ports:
      - "8000:8000"
    command: python backend/manage.py runserver 0.0.0.0:8000
    depends_on:
      - db
      - redis
    networks:
      - droptools_net
    # 6. Redis (Message Broker para Celery)
  redis:
    image: redis:7-alpine
    container_name: droptools_redis
    restart: always
    ports:
      - "6379:6379"
    networks:
      - droptools_net
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

  # 7. Celery Worker (Procesamiento Asíncrono de Reportes)
  celery_worker:
    build:
      context: .
      target: selenium
    container_name: droptools_celery_worker
    restart: always
    env_file:
      - .env.docker
    environment:
      - DROPTOOLS_ENV=development
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - PYTHONPATH=/app/backend
      # Salta Google y landing; va directo a app.dropi.co/auth/login (útil si el proxy recibe página en blanco de Google)
      - DROPI_LOGIN_DIRECT=1
      # DROPI_NO_PROXY=1 para probar sin proxy; quitado para usar proxy de proxy_dev_config.json
      # Solo navegadores estables en Linux: Chromium y Firefox (sin Edge)
      - BROWSER_ORDER=chrome,firefox
      - CHROME_BIN=/usr/bin/chromium
      - CHROMEDRIVER=/usr/bin/chromedriver
    volumes:
      - ./backend:/app/backend
      - ./logs:/app/logs
      - ./results:/app/backend/results
    working_dir: /app/backend
    command: celery -A droptools_backend worker -l info --concurrency=3
    depends_on:
      - redis
      - backend
    networks:
      - droptools_net
    dns:
      - 8.8.8.8
      - 1.1.1.1
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 6G
        reservations:
          cpus: '1.0'
          memory: 2G
    shm_size: '2g'

  # 8. Flower (Dashboard de Monitoreo para Celery)
  flower:
    build:
      context: .
      target: selenium
    container_name: droptools_flower
    restart: always
    env_file:
      - .env.docker
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    ports:
      - "5555:5555"
    working_dir: /app/backend
    command: celery -A droptools_backend flower --port=5555
    depends_on:
      - redis
      - celery_worker
    networks:
      - droptools_net
    # 9. Frontend (React + Vite)
  frontend:
    image: node:20-alpine
    container_name: droptools_frontend
    restart: always
    working_dir: /app
    volumes:
      - ./frontend:/app
      - /app/node_modules # Evitar conflictos con node_modules del host
    ports:
      - "5173:5173"
    environment:
      - CHOKIDAR_USEPOLLING=true # Necesario para hot-reload en Windows/Docker
    command: sh -c "npm install --legacy-peer-deps && npm run dev -- --host"
    networks:
      - droptools_net

volumes:
  pg_data: # Persistencia de la DB
  redis_data:
    # Persistencia de Redis

networks:
  droptools_net:
    driver: bridge
